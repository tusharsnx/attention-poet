{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit (windows store)"
  },
  "interpreter": {
   "hash": "9b7910a43f22d8687f0de6ecfce0c1865e7017eb5d643397fde905e2a9a5ff87"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "import numpy as np\r\n",
    "import tensorflow as tf\r\n",
    "import pandas as pd\r\n",
    "import tensorflow_text as tf_txt \r\n",
    "from typing import List, Dict"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "source": [
    "MAX_VOCAB_SIZE = 10000\r\n",
    "EMBEDDING_DIM = 200\r\n",
    "DFF = 512\r\n",
    "D_MODEL = 256\r\n",
    "MAX_SEQ_LEN = 10"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "def get_angles(pos, i, dims):\r\n",
    "  angle_rates = 1 / (10000 ** ((2 * (i//2)) / dims))\r\n",
    "  return pos * angle_rates"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "def positional_encoding(position, d_model):\r\n",
    "  angle_rads = get_angles(np.arange(position)[:, np.newaxis],\r\n",
    "                          np.arange(d_model)[np.newaxis, :],\r\n",
    "                          d_model)\r\n",
    "\r\n",
    "  # apply sin to even indices in the array; 2i\r\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\r\n",
    "\r\n",
    "  # apply cos to odd indices in the array; 2i+1\r\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\r\n",
    "\r\n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\r\n",
    "\r\n",
    "  return tf.cast(pos_encoding, dtype=tf.float32)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "output = positional_encoding(10, 200)\r\n",
    "output.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TensorShape([1, 10, 200])"
      ]
     },
     "metadata": {},
     "execution_count": 27
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def lookahead_mask(seq):\r\n",
    "    return 1 - tf.linalg.band_part(tf.ones((seq, seq)), -1, 0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "output = lookahead_mask(4)\r\n",
    "output.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TensorShape([4, 4])"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "source": [
    "class Preprocessor:\r\n",
    "    def __init__(self, vocab_size, seq_len=10):\r\n",
    "        self.seq_len = seq_len\r\n",
    "        self.vocab: List[str] = None\r\n",
    "        self.word_ids: Dict[str, int] = None\r\n",
    "        self.rev_word_ids:  Dict[int, str] = None\r\n",
    "        self.vocab_size = None\r\n",
    "        self.tokenizer = tf.keras.layers.experimental.preprocessing.TextVectorization(max_tokens=vocab_size,\r\n",
    "                                        output_sequence_length=self.seq_len, standardize=self.custom_standardize\r\n",
    "                                        )\r\n",
    "\r\n",
    "    def __call__(self, inputs):\r\n",
    "        encoded_seq = tf_txt.normalize_utf8(inputs, \"NFKD\")\r\n",
    "        tokenized_seq = self.tokenizer(self.add_extra(inputs))\r\n",
    "        return tokenized_seq\r\n",
    "    \r\n",
    "    @staticmethod\r\n",
    "    def add_extra(inputs):\r\n",
    "        inputs = tf.constant(inputs)\r\n",
    "        return [[\"[SURU] \"]]+inputs+[[\" [KHATAM]\"]]\r\n",
    "    \r\n",
    "    def custom_standardize(self, text):\r\n",
    "        return text\r\n",
    "\r\n",
    "    \r\n",
    "    def build_vocab(self, inputs):\r\n",
    "        self.tokenizer.adapt(self.add_extra(inputs))\r\n",
    "        self.vocab = self.tokenizer.get_vocabulary()\r\n",
    "        self.vocab_size = len(self.vocab)\r\n",
    "        self.build_dictionary(self.vocab)\r\n",
    "        return self.vocab\r\n",
    "\r\n",
    "    def build_dictionary(self, vocab_list: List[str]):\r\n",
    "        word_ids = dict()\r\n",
    "        rev_word_ids = dict()\r\n",
    "        for i, item in enumerate(vocab_list):\r\n",
    "            word_ids[item] = i\r\n",
    "            rev_word_ids[i] = item\r\n",
    "        self.word_ids = word_ids\r\n",
    "        self.rev_word_ids = rev_word_ids\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "source": [
    "preprocessor = Preprocessor(vocab_size=100, seq_len=10)\r\n",
    "inputs = [[\"जैसा \"], [\"i am fine, what about you. ? \"]]\r\n",
    "vocab = preprocessor.build_vocab(inputs)\r\n",
    "print(vocab)\r\n",
    "print(preprocessor(inputs))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['', '[UNK]', '[SURU]', '[KHATAM]', 'जैसा', 'you.', 'what', 'i', 'fine,', 'am', 'about', '?']\n",
      "tf.Tensor(\n",
      "[[ 2  4  3  0  0  0  0  0  0  0]\n",
      " [ 2  7  9  8  6 10  5 11  3  0]], shape=(2, 10), dtype=int64)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "class FFN(tf.keras.layers.Layer):\r\n",
    "  def  __init__(self, d_model, dff):\r\n",
    "        super().__init__()\r\n",
    "        self.dff = dff\r\n",
    "        self.dense1 = tf.keras.layers.Dense(dff, activation=\"relu\")\r\n",
    "        self.dense2 = tf.keras.layers.Dense(d_model)\r\n",
    "\r\n",
    "  def call(self, inputs):\r\n",
    "        outputs = self.dense1(inputs)\r\n",
    "        outputs = self.dense2(outputs)\r\n",
    "        return outputs\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "class Block(tf.keras.layers.Layer):\r\n",
    "    def __init__(self, d_model: int, dff: int = 2048, heads: int = 8, rate: int = 0.1):\r\n",
    "        super().__init__()\r\n",
    "\r\n",
    "        assert d_model%heads==0\r\n",
    "        #parameters\r\n",
    "        self.d_model = d_model      # model dims \r\n",
    "        self.dff = dff                           # ffn dense layer units\r\n",
    "        self.heads = heads               # number of heads\r\n",
    "\r\n",
    "        #layers\r\n",
    "        self.ffn = FFN(d_model, dff)\r\n",
    "        self.ln1 = tf.keras.layers.LayerNormalization()\r\n",
    "        self.ln2 = tf.keras.layers.LayerNormalization()\r\n",
    "        self.wq = tf.keras.layers.Dense(self.d_model)\r\n",
    "        self.wv = tf.keras.layers.Dense(self.d_model)\r\n",
    "        self.wi = tf.keras.layers.Dense(self.d_model)\r\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\r\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\r\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=self.heads, key_dim=self.d_model)\r\n",
    "\r\n",
    "    # def build(self, input_shape):\r\n",
    "    #     self.mha = tf.keras.layers.MultiHeadAttention(num_heads=self.heads, key_dim=d_model)\r\n",
    "\r\n",
    "    def call(self, inputs, training=False, mask=None):\r\n",
    "        q = self.wq(inputs)     #(None, seq_len, d_model)\r\n",
    "        v = self.wv(inputs)      #(None, seq_len, d_model)\r\n",
    "\r\n",
    "        # projecting on higher dimension to add with attention_outputs in  ln\r\n",
    "        inputs = self.wi(inputs)  #(None, seq_len, d_model)\r\n",
    "        attention_outputs = self.mha(query=q, value=v, attention_mask=mask)      # output shape (None, query_len, d_model)\r\n",
    "        dropped_attention_outputs = self.dropout1(attention_outputs, training=training)\r\n",
    "        outputs = self.ln1(inputs+dropped_attention_outputs)\r\n",
    "\r\n",
    "        ffn_outputs = self.ffn(outputs)     # output shape (None, query_len, d_model)\r\n",
    "        dropped_ffn_outputs = self.dropout1(ffn_outputs, training=training)\r\n",
    "        outputs = self.ln2(inputs+dropped_ffn_outputs)        # output shape (None, query_len, d_model)\r\n",
    "        \r\n",
    "        return outputs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "source": [
    "layer = Block(d_model=8, dff=256, heads=4)\r\n",
    "mask = tf.keras.Input(shape=[4, 4])\r\n",
    "source = tf.keras.Input(shape=[4, 100])\r\n",
    "outputs = layer(inputs=source, mask=mask)\r\n",
    "print(outputs.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(None, 4, 8)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "source": [
    "class Poet(tf.keras.models.Model):\r\n",
    "    def __init__(self, preprocessor, num_blocks=1, d_model=256, dff=512, heads=8, embedding_dims=100):\r\n",
    "        super().__init__()\r\n",
    "        self.d_model = d_model\r\n",
    "        self.preprocessor = preprocessor\r\n",
    "        self.num_blocks = num_blocks\r\n",
    "        self.embedding_dims = embedding_dims\r\n",
    "        # generating pos encoding now to save time while calling call()(as it is constant for all examples)\r\n",
    "        self.pos_encoding = positional_encoding(self.preprocessor.seq_len, self.embedding_dims)\r\n",
    "        self.embedding_layer = tf.keras.layers.Embedding(input_dim=self.preprocessor.vocab_size, \r\n",
    "                                            output_dim=self.embedding_dims, mask_zero=True, input_length=self.preprocessor.seq_len\r\n",
    "                                            )\r\n",
    "        self.blocks = [Block(d_model=self.d_model, dff=dff, heads=heads) for i in range(self.num_blocks)]\r\n",
    "\r\n",
    "        self.final_layer = tf.keras.layers.Dense(self.preprocessor.vocab_size, activation=\"softmax\")\r\n",
    "\r\n",
    "\r\n",
    "    def call(self, inputs):\r\n",
    "        embeddings = self.embedding_layer(inputs)\r\n",
    "\r\n",
    "        # adding positional encoding\r\n",
    "        x = embeddings + self.pos_encoding\r\n",
    "        \r\n",
    "        # generate lookahead mask\r\n",
    "        mask = lookahead_mask(self.preprocessor.seq_len)\r\n",
    "\r\n",
    "        # passing rich attention embedding through each block\r\n",
    "        for block in self.blocks:\r\n",
    "            x = block(x, mask=mask)\r\n",
    "        \r\n",
    "        outputs = self.final_layer(x)\r\n",
    "        \r\n",
    "        return outputs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "source": [
    "string_inputs = [[\"जैसा\"], [\"i am fine, what about you. ? \"]]\r\n",
    "preprocess_inputs = preprocessor(string_inputs)\r\n",
    "print(inputs.shape)\r\n",
    "poet = Poet(preprocessor=preprocessor)\r\n",
    "outputs = poet.call(preprocess_inputs)\r\n",
    "print(outputs.shape)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(2, 10)\n",
      "(2, 10, 12)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "source": [],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "TensorShape([2, 10, 512])"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "metadata": {}
  }
 ]
}