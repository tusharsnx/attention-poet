{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import tensorflow as tf\r\n",
    "from typing import Dict, List\r\n",
    "import numpy as np\r\n",
    "import pickle as pkl"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "EMBEDDING_DIMS=100\r\n",
    "MAX_VOCAB_SIZE = 10000\r\n",
    "MAX_NEG_SAMPLES = 3\r\n",
    "MAX_SEQ_LEN = MAX_NEG_SAMPLES+2     # +2 for target and context words within the sequence"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "class Preprocessor:\r\n",
    "    def __init__(self, max_vocab_size):\r\n",
    "        self.max_vocab_size = max_vocab_size\r\n",
    "        # self.seq_len = seq_len\r\n",
    "\r\n",
    "        # fills after adapt\r\n",
    "        self.vocab_size = None\r\n",
    "        self.vocab = None\r\n",
    "        self.word_ids: Dict[str, int] = None\r\n",
    "        self.rev_word_ids:  Dict[int, str] = None\r\n",
    "        \r\n",
    "        self.string_lookup = tf.keras.layers.experimental.preprocessing.StringLookup(max_tokens=self.max_vocab_size)\r\n",
    "    \r\n",
    "    # bypass or create custom standardize\r\n",
    "    def _custom_standardize(self, text):\r\n",
    "        ''' Implements custom standardizing strategy\r\n",
    "        '''\r\n",
    "        return text\r\n",
    "\r\n",
    "    # builds lookup layer's vocabulary(calls adapt())\r\n",
    "    def build_vocab(self, inputs) -> List[str]:\r\n",
    "\r\n",
    "        inputs = tf.constant(inputs, dtype=tf.string)\r\n",
    "        assert tf.rank(inputs)==2, \"inputs rank must be 2, add or reduce extra axis\"\r\n",
    "\r\n",
    "        self.string_lookup.adapt(inputs)\r\n",
    "        self.vocab = self.string_lookup.get_vocabulary()\r\n",
    "        self.vocab_size = self.string_lookup.vocabulary_size()\r\n",
    "        self._build_dictionary(self.vocab)\r\n",
    "\r\n",
    "    # utility to build word_ids\r\n",
    "    def _build_dictionary(self, vocab_list: List[str]) -> None:\r\n",
    "        word_ids = dict()\r\n",
    "        rev_word_ids = dict()\r\n",
    "        for i, item in enumerate(vocab_list):\r\n",
    "            word_ids[item] = i\r\n",
    "            rev_word_ids[i] = item\r\n",
    "        self.word_ids = word_ids\r\n",
    "        self.rev_word_ids = rev_word_ids\r\n",
    "\r\n",
    "    # \r\n",
    "    def __call__(self, inputs):\r\n",
    "        inputs = tf.constant(inputs, dtype=tf.string)\r\n",
    "        assert tf.rank(inputs)==2, \"inputs rank must be 2, add or reduce extra axis\"\r\n",
    "        int_tokens = self.string_lookup(inputs)                   # (None, num_tokens)\r\n",
    "        return int_tokens                            \r\n",
    "\r\n",
    "    \r\n",
    "    # get text back from seq like [[5,2,3,4,0,0,0]]\r\n",
    "    def get_text(self, seqs) -> List[List[str]]:\r\n",
    "        texts = []\r\n",
    "        for seq in seqs:\r\n",
    "            string = \"\"\r\n",
    "            for ids in seq:\r\n",
    "                if ids!=0:\r\n",
    "                    string+= \" \" + self.rev_word_ids[ids]\r\n",
    "                else:\r\n",
    "                    break\r\n",
    "            texts.append([string.strip(\" \")])\r\n",
    "        return tf.Tensor(texts, dtype=tf.string)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "with open(\"embedding_data.pkl\", \"rb\") as f:\r\n",
    "    inputs = pickle.load(f)\r\n",
    "    \r\n",
    "\r\n",
    "# inputs = inputs.reshape((-1, 1))\r\n",
    "x_train, y_train = np.array(inputs[0]), np.array(inputs[1])\r\n",
    "x_train.shape, y_train.shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "((180256, 2), (180256,))"
      ]
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "preprocessor = Preprocessor(max_vocab_size=MAX_VOCAB_SIZE)\r\n",
    "preprocessor.build_vocab(x_train)\r\n",
    "preprocessed_inputs = preprocessor(x_train)\r\n",
    "print(preprocessor.vocab[:3])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['', '[UNK]', 'है']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "preprocessed_inputs, y_train"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(180256, 2), dtype=int64, numpy=\n",
       " array([[ 358,    6],\n",
       "        [ 358, 1272],\n",
       "        [ 358,   99],\n",
       "        ...,\n",
       "        [3164, 1837],\n",
       "        [3164,    4],\n",
       "        [3164,  747]], dtype=int64)>,\n",
       " array([1, 1, 0, ..., 0, 1, 1]))"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "class Embedder(tf.keras.models.Model):\r\n",
    "    def __init__(self, preprocessor, embedding_dims=100):\r\n",
    "        super().__init__()\r\n",
    "        self.embedding_dims = embedding_dims\r\n",
    "        self.preprocessor = preprocessor\r\n",
    "        self.vocab_size = self.preprocessor.vocab_size\r\n",
    "        self.emb_word = tf.keras.layers.Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dims)\r\n",
    "        self.emb_contxt = tf.keras.layers.Embedding(input_dim=self.vocab_size, output_dim=self.embedding_dims)\r\n",
    "        \r\n",
    "    # def build(self, input_shape):\r\n",
    "    #     assert input_shape.rank==2\r\n",
    "    #     self.vocab_size = input_shape[1]\r\n",
    "    #     self.output_layer = tf.keras.layers.Dense(self.vocab_size, activation=\"softmax\")\r\n",
    "    #     print(self.vocab_size)\r\n",
    "\r\n",
    "    def call(self, inputs):\r\n",
    "        # print(inputs.numpy())\r\n",
    "        words = inputs[:, :-1]\r\n",
    "        contxts = inputs[:, -1:]\r\n",
    "        # print(words, contxts)\r\n",
    "        words_emb = self.emb_word(words)\r\n",
    "        contxts_emb = self.emb_contxt(contxts)\r\n",
    "        # print(words_emb, contxts_emb)\r\n",
    "        product = tf.multiply(words_emb, contxts_emb)\r\n",
    "        # print(product)\r\n",
    "        dots = tf.reduce_sum(product, axis=2)\r\n",
    "        # print(dots)\r\n",
    "        return dots"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_checkpoints = tf.keras.callbacks.ModelCheckpoint(\r\n",
    "    monitor=\"loss\",\r\n",
    "    filepath=\"chkpt/\",\r\n",
    "    save_best_only=True, \r\n",
    "    save_weights_only=True\r\n",
    ")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "model = Embedder(embedding_dims=EMBEDDING_DIMS, preprocessor=preprocessor)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "model.compile(loss=tf.keras.losses.BinaryCrossentropy(), optimizer=tf.keras.optimizers.Adam(), metrics=[\"accuracy\"], run_eagerly=True)\r\n",
    "model.fit(preprocessed_inputs, y_train, epochs=10, batch_size=32, callbacks=[model_checkpoints])"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "fit() got an unexpected keyword argument 'run_eagerly'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17524/2603466739.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBinaryCrossentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_eagerly\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocessed_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_eagerly\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: fit() got an unexpected keyword argument 'run_eagerly'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.load_weights(\"Untitled Folder/chkpt\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "embeddings = model.emb_word.weights[0].numpy()\r\n",
    "emb_dict = dict()\r\n",
    "for word, idx in preprocessor.word_ids.items():\r\n",
    "    if idx==0:\r\n",
    "        emb_dict[\"pad\"] = list(embeddings[idx])\r\n",
    "    else:\r\n",
    "        emb_dict[word] = list(embeddings[idx])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open(f\"embedding-{EMBEDDING_DIMS}.pkl\", \"wb\") as f:\r\n",
    "    pkl.dump(emb_dict, f)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit (windows store)"
  },
  "interpreter": {
   "hash": "9b7910a43f22d8687f0de6ecfce0c1865e7017eb5d643397fde905e2a9a5ff87"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}